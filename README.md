# Multimodal Sentiment Analysis using Transformers

This project focuses on building a **Multimodal Sentiment Analysis** model that leverages **text**, **image**, and potentially **audio** data to predict sentiment with higher accuracy than unimodal systems. The project uses **Transformer-based architectures** and is part of a six-week winter project organized by the **Google Developers Group (GDG) at IIT Kanpur**.

## ğŸ“Œ Objective

To develop a deep learning model that integrates multiple modalities â€” such as **textual**, **visual**, and possibly **audio** data â€” for more accurate sentiment prediction.

---

## ğŸ›  Tech Stack & Tools

### Programming Language
- Python

### Libraries & Frameworks
- ğŸ¤— Hugging Face Transformers
- PyTorch
- Torchvision
- NumPy
- Pandas
- Scikit-learn
- Matplotlib
- Seaborn

### Tools
- Jupyter Notebook
- Google Colaboratory
- Kaggle
- Hugging Face Datasets

---

## ğŸ§  Model Architecture

- **Text Encoder**: BERT
- **Image Encoder**: RESNET
- **Fusion Strategy**: Early Fusion
- **Classifier Head**: Fully Connected Layers

---


## ğŸ“š Learning Outcomes

- Understanding of multimodal data processing
- Working with Transformer-based architectures
- Model fusion techniques
- Performance analysis and visualization

---


## ğŸ¤ Acknowledgments

Special thanks to:
- **Google Developers Group, IIT Kanpur**
- Mentors Sneha Barman(snehabarman04) and Param Soni(param-gh)
